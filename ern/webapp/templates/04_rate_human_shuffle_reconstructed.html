<h2>Sample - {{ sample_id }} </h2>

<h2>Task 3: Rating the model's reconstruction of your shuffle</h2>
A Large Language Model has attempted to reconstruct your story.

Your task is to rate how well the model has done so on three points:

<h4>Flow</h4>
How well has the model reconstructed the story?
A score of 0 means that the model has made many mistakes or produced garbage output, while a score of 10 means that the model has successfully reconstructed the logical ordering of the story.

<h4>Fluency</h4>
It is possible that the model rewrote (some parts of) the story in a way that is grammatically incorrect or awkward.
A score of 0 means that the model has made many mistakes or produced garbage output.
A score of 10 means that the model has made minimal modifications, perhaps appropriately correcting a typo or two.

<h4>Accuracy</h4>
The factual accuracy of the story. That is, did the model add or remove information that was not present in the original story?
A score of 0 means that the model has fabricated information, while a score of 10 means that the model has not added or removed any information.

<h2>Your original story</h2>
{{ story_text }}

<h2>The LLM's reconstruction</h2>
{{ human_reconstructed_story }}

<form action="{% url 'rate_human_shuffle_reconstructed' %}" method="post">
    {% csrf_token %}
    {{ form }}
    <input type="submit" value='{{ _("Next") }}'>
</form>

<a href="{% url 'index' %}">{{ _("End experiment") }}</a>